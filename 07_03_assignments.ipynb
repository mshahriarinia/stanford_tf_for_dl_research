{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "\n",
    "## Deep Dream\n",
    "Take an image, pass it through multiple layers of an image classification model which is prtrained on imagenet (e.g. inception). final layers of these networks are great at capturing imherent and abstract features of images. \n",
    "\n",
    "1. Feed the image through these layers\n",
    "2. Take the output of one of the final layers\n",
    "3. Calculate its gradient (w.r.t.) on the initial input variable (input image)\n",
    "4. __Gradient Ascent!:__ It calculates the gradient of the given layer of the Inception model with regard to the input image. The gradient is then added to the input image so the mean value of the layer-tensor is increased. This process is repeated a number of times and amplifies whatever patterns the Inception model sees in the input image. ([Awesome Youtube Video](https://www.youtube.com/watch?v=ws-ZbiFV1Ms). and its [python notebook](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/14_DeepDream.ipynb)). Basically it magnifies the gradients (textures) while preserving colors, hence gradient ascent!\n",
    "\n",
    "## Style Transfer\n",
    "Weighted combination of the result of Deep dream of two images.\n",
    "\n",
    "1. mean-squared-error between content image and random image to make them look like each other\n",
    "2. calculate the gradient ascent of style image but add it to random image.\n",
    "\n",
    "image gets the gradients from one image, colors from another image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Content/style of an image\n",
    "\n",
    "Feature visualization have shown that:\n",
    "- lower layers extract features related to content\n",
    "- higher layers extract features related to style\n",
    "\n",
    "### Content loss\n",
    "To measure the content loss between the feature map in the\n",
    "content layer of the generated image and the content image\n",
    "Paper: ‘conv4_4’\n",
    "### Style loss\n",
    "To measure the style loss between the gram matrices of feature\n",
    "maps in the style layers of the generated image and the style\n",
    "image\n",
    "Paper: [‘conv1_1’, ‘conv2_1’, ‘conv3_1’, ‘conv4_1’ and ‘conv5_1’]\n",
    "\n",
    "Give more weight to deeper layers\n",
    "E.g. 1.o for ‘conv1_1’, 2.0 for ‘conv2_1’, ...\n",
    "\n",
    "Optimizes the initial image to minimize the combination of the two losses. Do not optimize the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricky implementation details\n",
    "1. Train input instead of weights\n",
    "2. Multiple tensors share the same variable to avoid assembling identical subgraphs\n",
    "a. Content image\n",
    "b. Style image\n",
    "c. Initial image\n",
    "3. Use pre-trained weights (from VGG-19)\n",
    "a. Weights and biases already loaded for you\n",
    "b. They are numpy, so need to be converted to tensors\n",
    "c. Must not be trainable!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
