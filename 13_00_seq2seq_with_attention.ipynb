{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original idea comes from paper: \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"\n",
    "\n",
    "Encode and decode RNNs (Often GRU or LSTM is used)\n",
    "\n",
    "## Sequence to Sequence\n",
    "\n",
    "Consists of two recurrent neural networks (RNNs):\n",
    "1. Encoder maps a variable-length source sequence (input) to a fixed-length vector\n",
    "2. Decoder maps the vector representation back to a variable-length target sequence (output)\n",
    "\n",
    "Two RNNs are trained jointly to maximize the conditional probability of the target sequence given a source sequence\n",
    "\n",
    "Vanila Encoder-Decoder RNN\n",
    "<img src=\"img/encoder_decoder.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Encoder-Decoder RNN for en2de Translation \n",
    "<img src=\"img/encoder_decoder_en2de.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Encoder-Decoder RNN with Attention\n",
    "<img src=\"img/encoder_decoder_attention.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket Similar Sequence Lengthes Together\n",
    "\n",
    "- Avoid too much padding that leads to extraneous computation\n",
    "- Group sequences of similar lengths into the same buckets\n",
    "- Create a separate subgraph for each bucket\n",
    "- In theory, can use for v1.0:\n",
    "\n",
    "      tf.contrib.training.bucket_by_sequence_length(max_length,\n",
    "      examples, batch_size, bucket_boundaries, capacity=2 *\n",
    "      batch_size, dynamic_pad=True)\n",
    "\n",
    "- In practice, use the bucketing algorithm used in TensorFlow’s\n",
    "translate model (because we’re using v0.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Loss\n",
    "\n",
    "Do you want to train a multiclass or multilabel model with thousands or millions of output classes (for example, a language model with a large vocabulary)? Training with a full Softmax is slow in this case, since all of the classes are evaluated for every training example. Candidate Sampling training algorithms can speed up your step times by only considering a small randomly-chosen subset of contrastive classes (called candidates) for each batch of training examples.\n",
    "\n",
    "Among options are NCE (noise-contrastive estimation) loss, sampled softmax loss, \n",
    "\n",
    "### Sampled Softmax\n",
    "This is a faster way to train a softmax classifier over a huge number of classes. \n",
    "- This operation is for training only. It is generally an underestimate of the full softmax loss. \n",
    "- At inference time, you can compute full softmax probabilities with the expression tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases).\n",
    "\n",
    "Notes:\n",
    "- Avoid the growing complexity of computing the normalization constant\n",
    "- Approximate the negative term of the gradient, by importance sampling with a small number of samples.\n",
    "- At each step, update only the vectors associated with the correct word w and with the sampled words in V’\n",
    "- Once training is over, use the full target vocabulary to compute the output probability of each target word \n",
    "\n",
    "Also:\n",
    "- Generally an underestimate of the full softmax loss.\n",
    "- At inference time, compute the full softmax using:\n",
    "  - tf.nn.softmax(tf.matmul(inputs, tf.transpose(weight)) + bias)\n",
    "\n",
    "[1] On Using Very Large Target Vocabulary for Neural Machine Translation (Jean et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example use of sampled_softmax_loss\n",
    "if config.NUM_SAMPLES > 0 and config.NUM_SAMPLES < config.DEC_VOCAB:\n",
    "        weight = tf.get_variable('proj_w', [config.HIDDEN_SIZE, config.DEC_VOCAB])\n",
    "        bias = tf.get_variable('proj_b', [config.DEC_VOCAB])\n",
    "        self.output_projection = (w, b)\n",
    "    def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        return tf.nn.sampled_softmax_loss(tf.transpose(weight), bias, inputs, labels,\n",
    "        config.NUM_SAMPLES, config.DEC_VOCAB)\n",
    "    self.softmax_loss_function = sampled_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary tradeoff\n",
    "- Get all tokens that appear at least a number of time (twice)\n",
    "- Alternative approach: get a fixed size vocabulary\n",
    "\n",
    "Smaller vocabulary:\n",
    "- Has smaller loss/perplexity but loss/perplexity isn’t everything\n",
    "- Gives < unk> answers to questions that require personal information\n",
    "- Doesn’t give the bot’s answers much response\n",
    "- Doesn’t train much faster than big vocab using sampled softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "How do we know that we implemented our model correctly?\n",
    "- Run the model on a small dataset (~2,000 pairs) \n",
    "- and run for a lot of epochs to see if it converges\n",
    "- (learns all the responses by heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "- The bot is very dramatic (thanks to Hollywood screenwriters)\n",
    "- Topics of conversations aren’t realistic\n",
    "- Responses are always fixed for one encoder input\n",
    "- Inconsistent personality\n",
    "- Use only the last previous utterance as the input for the encoder\n",
    "- Doesn’t keep track of information about users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Please review these slides one more time! http://web.stanford.edu/class/cs20si/lectures/slides_13.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
