{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up Personality\n",
    "\n",
    "A) Let bot remember personal information about the user\n",
    "- At the decoder phase, inject consistent information about the bot such as name, age, hometown, current location, job.\n",
    "\n",
    "B) Maintain same personality theme for bot\n",
    "- Use the decoder inputs from one character only. For example: your own Sheldon Cooper bot!\n",
    "- [Plenty of good suggestions on Quora](https://www.quora.com/How-do-you-design-the-personality-of-a-chatbot)\n",
    "\n",
    "C) Use character-level sequence to sequence model for the chatbot\n",
    "- We’ve built a character-level language model and it seems to be working pretty well.\n",
    "- An obvious advantage of this model is that it uses a much smaller vocabulary so we can use full softmax instead of sampled softmax, and there will be no unknown tokens! \n",
    "- An obvious disadvantage is that the sequence will be much longer \n",
    "  - it’ll be approximately 4 times longer than the token-level one.\n",
    " \n",
    "D) Construct the response in a non-greedy way\n",
    "- This greedy approach works poorly, and restricts the chatbot to give one fixed answer to a input. For example, if the user says “hi”, the bot will always “hi” back, while in real life, people can vary their responses to “hey”, “how are you?”, or “hi. what’s up?”\n",
    "- You can try to use viterbi or beam search to construct the most probable response.\n",
    "\n",
    "E)  Create a feedback loop that allows users to train your chatbot\n",
    "- Create a feedback loop so that users can help the bot learn the right response -- treat the bot like a baby. So when the bot says something incorrect, users can say: “That’s wrong. You should have said xyz.” and the bot will correct its response to xyz.\n",
    "- It can be dangerous because users are mean and can turn your chatbot into something utterly racist and sexist. Microsoft did that for their chatbot Tay and [see what happened](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- There isn’t any scientific method to measure the human-like quality of speech. \n",
    "- The matter is made even more complicated when we have humans that talk like bots.\n",
    "- The loss we report is the approximate softmax loss, and it means absolutely nothing in term of conversations. \n",
    "  - For example, if you convert every token to < unk> and always construct response as a series of < unk> tokens, then your loss would be 0.\n",
    "  \n",
    "Let humans be the judge of the quality!\n",
    "-  Each person/team will have 5 minutes, of which 2 minutes to introduce themselves and demo their chatbots, then the rest of the class can try play with their chatbots. The class will vote for their favorite chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "A) Know thy data\n",
    "- You should know your dataset very well so that you can do the suitable data pre-processing and to see the characteristics you can expect from this dataset.\n",
    "\n",
    "B) Adjust the learning rate\n",
    "- You should pay attention to the reported loss and adjust the learning rate accordingly. Please read the CS231N note on how to read your learning rate.\n",
    "- Keep in mind that each bucket has its own optimizer, so you can have different learning rates for different buckets. \n",
    "  - For example, buckets with a larger size might need a slightly larger learning rate.\n",
    "- You should feel free to experiment with other optimizers other than SGD.\n",
    "\n",
    "C) Let your friends try the bot\n",
    "- You can learn a lot about how humans interact with bots when you let your friends try your bot,\n",
    "- you can use that information to make your bot more human-like.\n",
    "\n",
    "D) Don’t be afraid of handcrafted rules\n",
    "- Sometimes, you’ll have to resort to handcrafted rules. \n",
    "- For example, if the generated response is just empty, then instead of having the bot saying nothing, you can say   \n",
    "  - “I don’t know what to say.” \n",
    "  - “I don’t understand what you just said.” \n",
    "  - “Tell me about something else.” \n",
    "- This will make the conversation flows a lot more naturally.\n",
    "5. Have fun!\n",
    "- This assignment is supposed to be fun. \n",
    "- Don’t get disheartened if your bot seems to just talk gibberish \n",
    "- even famous bots made by companies with vast resources like Apple or Google give nonsensical responses most of the time.\n",
    "- It’ll take a long time to train. \n",
    "  - For a batch of 64, \n",
    "    - it takes 1.2 - 2.2s/step on a GPU, \n",
    "    - on a CPU it’s about 4x slower with 3.8 - 7.5s/step. \n",
    "  - On a GPU, it’d take *an hour* to train an epoch for a train set of 100,000 samples, \n",
    "  - you’d need to train for at least 3-4 epochs before your bot starts to make sense. \n",
    "  - Plan your time accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
