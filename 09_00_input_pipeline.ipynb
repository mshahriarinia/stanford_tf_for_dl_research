{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "__TFRecord__ is TensorFlow's binary data format, which is a serialized __tf.train.Example__ *Protobuf object*.\n",
    "\n",
    "__Protobuf (Protocol Buffers)__ is a method of serializing structured data like Thrift. Designed to be smaller and faster than XML. To use:  define data structures (called messages) and services in a proto definition file (.proto) and compiles it with *protoc*.\n",
    "- there is no way to tell the names, meaning, or full datatypes of fields without an external specification e.g. ASCII serialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example encode and decode an images dataset\n",
    "Encode:\n",
    "1. Create a TFRecord file writer\n",
    "2. Convert image to bytes\n",
    "3. Create an instance of tf.train.Example (which is a TFRecord) and add label, shape, and image content to it.\n",
    "4. Write via TFRecord file writer\n",
    "\n",
    "Decode:\n",
    "1. Create a queue of all files to be read\n",
    "2. Create a TFRecord reader\n",
    "3. Read from queue\n",
    "4. Specify and parse feature types of the example\n",
    "5. Cast each feature to proper types\n",
    "6. Apply other characteristics that you already should know about the data such as shape\n",
    "\n",
    "Keep in mind that label, shape, and image returned are tensor objects. To get their values, you’ll have to eval them in tf.Session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## ENCODE\n",
    "# First, we need to read in the image and convert it to byte string\n",
    "def get_image_binary(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = np.asarray(image, np.uint8)\n",
    "    shape = np.array(image.shape, np.int32)\n",
    "return shape.tobytes(), image.tobytes() # convert image to raw data bytes in the array.\n",
    "\n",
    "def write_to_tfrecord (label, shape, binary_image, tfrecord_file):\n",
    "    \"\"\" Write a single sample to TFRecord file, to write more samples, just use a loop!\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(tfrecord_file)  # Create a TFRecord writer\n",
    "    # Create an instance of tf.train.Example (which is a TFRecord) and add label, shape, and image content to it\n",
    "    example=tf.train.Example(features=tf.train.Features(feature={ )\n",
    "        'label': tf.train.Feature(bytes_list=tf.train.BytesList(value = [label])),\n",
    "        'shape': tf.train.Feature(bytes_list=tf.train.BytesList(value = [shape])),\n",
    "        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value = [binary_image]))\n",
    "        }))\n",
    "    # write via TFRecordfile writer\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "#########################################################################\n",
    "## DECODE\n",
    "#\n",
    "def read_from_tfrecord(filenames): \n",
    "    # create a queue of all files to be read\n",
    "    tfrecord_file_queue = tf.train.string_input_producer(filenames, name = 'queue') \n",
    "    # Create a TFRecord reader\n",
    "    reader = tf.TFRecordReader() \n",
    "    # Read from queue\n",
    "    _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "    # label and image are stored as bytes but could be stored as int64/float64 values in a serialized tf.Exampleprotobuf\n",
    "    tfrecord_features = tf.parse_single_example(tfrecord_serialized, \n",
    "        features = {\n",
    "            'label': tf.FixedLenFeature([], tf.string), \n",
    "            'shape': tf.FixedLenFeature([], tf.string), \n",
    "            'image': tf.FixedLenFeature([], tf.string), \n",
    "        }, name = 'features')\n",
    "    # image was saved as uint8, so we have to decode as uint8.\n",
    "    image = tf.decode_raw(tfrecord_features['image'], tf.uint8)\n",
    "    shape = tf.decode_raw(tfrecord_features['shape'], tf.int32)\n",
    "    # the image tensor is flattened out, so we have to reconstruct the shape\n",
    "    image = tf.reshape(image, shape)\n",
    "    label = tf.cast(tfrecord_features['label'], tf.string)\n",
    "    return label, shape, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threading in Tensorflow\n",
    "\n",
    "- Multiple threads prepare training examples and push them in the queue.\n",
    "- A training thread executes a training op that dequeues mini-batches from the queue.\n",
    "\n",
    "Session object is designed multithreaded, so multiple threads can easily use the same session and run ops in parallel.\n",
    "\n",
    "TensorFlow provides two classes to help with the threading: *tf.Coordinator* and *tf.train.QueueRunner*. These two classes are designed to be used together. \n",
    "\n",
    "- __Coordinator__ \n",
    "  - helps multiple threads stop together \n",
    "  - report exceptions to a program that waits for them to finish. \n",
    "\n",
    "- __QueueRunner__ is used to create a number of threads cooperating to enqueue tensors in the same queue. Methods they provide: enqueue, enqueue_many, and dequeue\n",
    "    - tf.FIFOQueue: a queue the dequeues elements in a first in first out order\n",
    "    - tf.RandomShuffleQueue: dequeues elements in, a random order\n",
    "    - tf.PaddingFIFOQueue: FIFOQueue that supports batching variable-sized tensors by padding\n",
    "    - tf.PriorityQueue: FIFOQueue whose enqueues and dequeues take in another argument: the priority.\n",
    "\n",
    "dequeue_many is not allowed. If you want to get multiple elements at once for your batch training, you’ll have to use *tf.train.batch* or *tf.train.shuffle_batch* if you want to your batch to be shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example Coordinator+Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "N_SAMPLES = 1000\n",
    "NUM_THREADS = 4\n",
    "\n",
    "# Generating simple data\n",
    "# create random samples with 4 features and random binary labels (0/1)\n",
    "data = np.random.randn(N_SAMPLES, 4)\n",
    "label = np.random.randint(0, 2, size = N_SAMPLES)\n",
    "\n",
    "# Define queue\n",
    "queue = tf.FIFOQueue(capacity = 50, dtypes =[tf.float32, tf.int32], shapes =[[4], []])\n",
    "# Define enqueue and deqeue op\n",
    "enqueue_op = queue.enqueue_many([data, label])\n",
    "data_sample_op, lable_sample_op = queue.dequeue() # dequeue op\n",
    "\n",
    "# Define QueueRunner on the queue with NUM_THREADS to do *enqueue* op\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\n",
    "with tf.Session() as sess:\n",
    "    # Create a thread coordinator, launch the queue runner threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    # Create threads (enqueue) and assign their coordinator to the one just created.\n",
    "    enqueue_threads = qr.create_threads(sess, coord = coord, start = True)\n",
    "    for step in xrange(100): # do to 100 iterations\n",
    "        if coord.should_stop():\n",
    "            break\n",
    "        # dequeue one\n",
    "        data_batch, label_batch = sess.run([data_sample_op, lable_sample_op]) # dequeue ops can be used as placeholders\n",
    "        #print(data_batch)\n",
    "        #print('--')\n",
    "    coord.request_stop()\n",
    "    coord.join(enqueue_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using Coordinate to manage normal multi-threading in python\n",
    "Taken from [Tensorflow Threading_and_Queues](https://www.tensorflow.org/programmers_guide/threading_and_queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using tf.Coordinator() for normal threads\n",
    "\n",
    "def my_loop (coord):\n",
    "    \"\"\"\n",
    "    thread body: loop until the coordinator indicates a stop was requested.\n",
    "    if some condition becomes true, ask the coordinator to stop.\n",
    "    \"\"\"\n",
    "    while not coord.should_stop():\n",
    "        ... do something ...\n",
    "    if ... some condition ...:\n",
    "        coord.request_stop()\n",
    "\n",
    "import threading\n",
    "# main code: create a coordinator.\n",
    "coord = tf.Coordinator()\n",
    "# create 10 threads that run 'my_loop()'\n",
    "# you can also create threads using QueueRunner as the example above\n",
    "threads = [threading.Thread (target = my_loop, args =(coord,)) for _ in xrange (10)]\n",
    "# start the threads and wait for all of them to stop.\n",
    "for t in threads :\n",
    "    t.start()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "To read in data\n",
    "- Constants: will seriously bloat your graph -- which you’ll see in assignment 2.\n",
    "- Feed dict: which has the drawback of first loading the data from storage to the client and then from the client to workers, which can be slow especially when the client and workers are on different machines. \n",
    "- Data Readers: use data readers to load your data directly from storage to workers. In theory, this means that you can load in an amount of data limited only by your storage and not your device.\n",
    "  - __TextLineReader:__ Outputs the lines of a file delimited by newlines. e.g. text files, CSV files\n",
    "  - __FixedLengthRecordReader:__ Outputs the entire file when all files have same fixed lengths. e.g. each MNIST file has 28 x 28 pixels, CIFAR - 10 32 x 32 x 3\n",
    "  - __WholeFileReader:__ Outputs the entire file content. This is useful when each file contains a sample\n",
    "  - __TFRecordReader:__ Reads samples from TFRecord files (TensorFlow's own binary format)\n",
    "  - __ReaderBase:__ Allows you to create your own readers\n",
    " \n",
    "\n",
    "To use data reader, we first need to create a queue to hold the names of all the files you want to read in through __string_input_producer__ which is a queue of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"heart.csv\"])\n",
    "reader = tf.TextLineReader(skip_header_lines=1) # skip header row\n",
    "key, val = reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reader:__ My friend encouraged me to think of readers as ops that return a different value every time you\n",
    "call it -- similar to Python generators. So when you call reader.read(), it’ll return you a pair key,\n",
    "value, in which key is a key to identify the file and record (useful for debugging if you have some\n",
    "weird records), and a scalar string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"heart.csv\"])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for _ in range(1): # generate 1 example\n",
    "        key, value = sess.run([key, value])\n",
    "        print value # 144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\n",
    "        print key # data/heart.csv:2\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
