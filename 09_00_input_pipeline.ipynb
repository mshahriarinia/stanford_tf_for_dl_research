{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "__TFRecord__ is TensorFlow's binary data format, which is a serialized __tf.train.Example__ *Protobuf object*.\n",
    "\n",
    "__Protobuf (Protocol Buffers)__ is a method of serializing structured data like Thrift. Designed to be smaller and faster than XML. To use:  define data structures (called messages) and services in a proto definition file (.proto) and compiles it with *protoc*.\n",
    "- there is no way to tell the names, meaning, or full datatypes of fields without an external specification e.g. ASCII serialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example encode and decode an images dataset\n",
    "Encode:\n",
    "1. Create a TFRecord file writer\n",
    "2. Convert image to bytes\n",
    "3. Create an instance of tf.train.Example (which is a TFRecord) and add label, shape, and image content to it.\n",
    "4. Write via TFRecord file writer\n",
    "\n",
    "Decode:\n",
    "1. Create a queue of all files to be read\n",
    "2. Create a TFRecord reader\n",
    "3. Read from queue\n",
    "4. Specify and parse feature types of the example\n",
    "5. Cast each feature to proper types\n",
    "6. Apply other characteristics that you already should know about the data such as shape\n",
    "\n",
    "Keep in mind that label, shape, and image returned are tensor objects. To get their values, you’ll have to eval them in tf.Session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## ENCODE\n",
    "# First, we need to read in the image and convert it to byte string\n",
    "def get_image_binary(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = np.asarray(image, np.uint8)\n",
    "    shape = np.array(image.shape, np.int32)\n",
    "return shape.tobytes(), image.tobytes() # convert image to raw data bytes in the array.\n",
    "\n",
    "def write_to_tfrecord (label, shape, binary_image, tfrecord_file):\n",
    "    \"\"\" Write a single sample to TFRecord file, to write more samples, just use a loop!\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(tfrecord_file)  # Create a TFRecord writer\n",
    "    # Create an instance of tf.train.Example (which is a TFRecord) and add label, shape, and image content to it\n",
    "    example=tf.train.Example(features=tf.train.Features(feature={ )\n",
    "        'label': tf.train.Feature(bytes_list=tf.train.BytesList(value = [label])),\n",
    "        'shape': tf.train.Feature(bytes_list=tf.train.BytesList(value = [shape])),\n",
    "        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value = [binary_image]))\n",
    "        }))\n",
    "    # write via TFRecordfile writer\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "#########################################################################\n",
    "## DECODE\n",
    "#\n",
    "def read_from_tfrecord(filenames): \n",
    "    # create a queue of all files to be read\n",
    "    tfrecord_file_queue = tf.train.string_input_producer(filenames, name = 'queue') \n",
    "    # Create a TFRecord reader\n",
    "    reader = tf.TFRecordReader() \n",
    "    # Read from queue\n",
    "    _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "    # label and image are stored as bytes but could be stored as int64/float64 values in a serialized tf.Exampleprotobuf\n",
    "    tfrecord_features = tf.parse_single_example(tfrecord_serialized, \n",
    "        features = {\n",
    "            'label': tf.FixedLenFeature([], tf.string), \n",
    "            'shape': tf.FixedLenFeature([], tf.string), \n",
    "            'image': tf.FixedLenFeature([], tf.string), \n",
    "        }, name = 'features')\n",
    "    # image was saved as uint8, so we have to decode as uint8.\n",
    "    image = tf.decode_raw(tfrecord_features['image'], tf.uint8)\n",
    "    shape = tf.decode_raw(tfrecord_features['shape'], tf.int32)\n",
    "    # the image tensor is flattened out, so we have to reconstruct the shape\n",
    "    image = tf.reshape(image, shape)\n",
    "    label = tf.cast(tfrecord_features['label'], tf.string)\n",
    "    return label, shape, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "N_SAMPLES = 1000\n",
    "NUM_THREADS = 4\n",
    "# Generating some simple data\n",
    "# create 1000 random samples, each is a 1D array from the normal distribution (10, 1)\n",
    "data = 10 * np . random . randn ( N_SAMPLES , 4 ) + 1\n",
    "# create 1000 random labels of 0 and 1\n",
    "target = np . random . randint ( 0 , 2 , size = N_SAMPLES )\n",
    "queue = tf . FIFOQueue ( capacity = 50 , dtypes =[ tf . float32 , tf . int32 ], shapes =[[ 4 ], []])\n",
    "enqueue_op = queue . enqueue_many ([ data , target ])\n",
    "dequeue_op = queue . dequeue ()\n",
    "# create NUM_THREADS to do enqueue\n",
    "qr = tf . train . QueueRunner ( queue , [ enqueue_op ] * NUM_THREADS)\n",
    "with tf . Session () as sess:\n",
    "    # Create a coordinator, launch the queue runner threads.\n",
    "    coord = tf . train . Coordinator ()\n",
    "    enqueue_threads = qr . create_threads ( sess , coord = coord , start = True)\n",
    "    for step in xrange ( 100 ): # do to 100 iterations\n",
    "        if coord . should_stop ():\n",
    "            break\n",
    "        data_batch , label_batch = sess . run ( dequeue_op)\n",
    "    coord . request_stop ()\n",
    "    coord . join ( enqueue_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of using tf.Coordinator() for normal threads\n",
    "\n",
    "import threading\n",
    "# thread body: loop until the coordinator indicates a stop was requested.\n",
    "# if some condition becomes true, ask the coordinator to stop.\n",
    "def my_loop ( coord ):\n",
    "while not coord . should_stop ():\n",
    "... do something ...\n",
    "if ... some condition ...:\n",
    "coord . request_stop ()\n",
    "# main code: create a coordinator.\n",
    "coord = tf . Coordinator ()\n",
    "# create 10 threads that run 'my_loop()'\n",
    "# you can also create threads using QueueRunner as the example above\n",
    "threads = [ threading . Thread ( target = my_loop , args =( coord ,)) for _ in xrange ( 10 )]\n",
    "# start the threads and wait for all of them to stop.\n",
    "for t in threads :\n",
    "t . start ()\n",
    "coord . join ( threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve learned that there are 3 different ways to read in data for your TensorFlow. The first is\n",
    "through constants (which will seriously bloat your graph -- which you’ll see in assignment 2).\n",
    "The second is through feed dict which has the drawback of first loading the data from storage to\n",
    "the client and then from the client to workers, which can be slow especially when the client and\n",
    "workers are on different machines. A common practice is to use data readers to load your data\n",
    "directly from storage to workers. In theory, this means that you can load in an amount of data\n",
    "limited only by your storage and not your device.\n",
    "\n",
    "There are several built-in readers for several common data types. The most versatile one is\n",
    "TextLineReader, which will read in any file delimited by newlines and will just return a line in\n",
    "that with each call. There are also a reader to read in files of fixed length, a reader to read in\n",
    "entire files, and a reader to read in the file of the type TFRecord (which we will go into below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf . TextLineReader\n",
    "Outputs the lines of a file delimited by newlines\n",
    "E . g . text files , CSV files\n",
    "\n",
    "tf . FixedLengthRecordReader\n",
    "Outputs the entire file when all files have same fixed lengths\n",
    "E . g . each MNIST file has 28 x 28 pixels , CIFAR - 10 32 x 32 x 3\n",
    "\n",
    "tf . WholeFileReader\n",
    "Outputs the entire file content. This is useful when each file contains a sample\n",
    "\n",
    "tf . TFRecordReader\n",
    "Reads samples from TensorFlow ' s own binary format ( TFRecord)\n",
    "\n",
    "tf . ReaderBase\n",
    "Allows you to create your own readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use data reader, we first need to create a queue to hold the names of all the files you want to\n",
    "read in through tf.train.string_input_producer.\n",
    "\n",
    "filename_queue = tf . train . string_input_producer ([ \"heart.csv\" ])\n",
    "reader = tf . TextLineReader (skip_header_lines=1)\n",
    "\n",
    " it means you choose to skip the first line for every file in the queue\n",
    "\n",
    "My friend encouraged me to think of readers as ops that return a different value every time you\n",
    "call it -- similar to Python generators. So when you call reader.read(), it’ll return you a pair key,\n",
    "value, in which key is a key to identify the file and record (useful for debugging if you have some\n",
    "weird records), and a scalar string value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ByteString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t',\n",
       "      dtype='|S48')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array(b)\n",
    "print(c)\n",
    "\n",
    "dt = np.dtype(int)\n",
    "np.frombuffer(b, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 7, 8, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = np.dtype(int)\n",
    "np.frombuffer(b, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
