{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cell Support (tf.nn.rnn_cell)__\n",
    "- BasicRNNCell: The most basic RNN cell.\n",
    "- RNNCell: Abstract object representing an RNN cell.\n",
    "- BasicLSTMCell: Basic LSTM recurrent network cell.\n",
    "- LSTMCell: LSTM recurrent network cell.\n",
    "- GRUCell: Gated Recurrent Unit cell \n",
    "\n",
    "Example:\n",
    "\n",
    "To construct Cells (tf.nn.rnn_cell)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    \n",
    "To stack multiple cells\n",
    "\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "Construct Recurrent Neural Network\n",
    "- tf.nn.dynamic_rnn: uses a tf.While loop to dynamically construct the graph when it is executed. Graph creation is faster and you can feed batches of variable size.\n",
    "- tf.nn.bidirectional_dynamic_rnn: dynamic_rnn with bidirectional\n",
    "\n",
    "Stack multiple cells\n",
    "\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)\n",
    "\n",
    "The problem with this is that you need to specify the *length*. However, most sequences are not of the same length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with variable sequence length\n",
    "\n",
    "*The padded labels change the total loss, which affects the gradients*\n",
    "\n",
    "Approach 1:\n",
    "  1. Maintain a mask (True for real, False for padded tokens)\n",
    "  2. Run your model on both the real/padded tokens (model will predict labels for the padded tokens as well)\n",
    "  3. Only take into account the loss caused by the real elements\n",
    "\n",
    "Example\n",
    "\n",
    "    full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)\n",
    "    loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))\n",
    "\n",
    "Approach 2: Let your model know the real sequence length so it only predict the labels for the real tokens\n",
    "\n",
    "Example\n",
    "\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "    tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with common problems when training RNNS\n",
    "\n",
    "### Vanishing Gradients\n",
    "Use different activation units:\n",
    "- tf.nn.relu\n",
    "- tf.nn.relu6\n",
    "- tf.nn.crelu\n",
    "- tf.nn.elu\n",
    "\n",
    "In addition to:\n",
    "- tf.nn.softplus\n",
    "- tf.nn.softsign\n",
    "- tf.nn.bias_add\n",
    "- tf.sigmoid\n",
    "- tf.tanh\n",
    "\n",
    "## Exploding Gradients\n",
    "Clip gradients with tf.clip_by_global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(cost, tf.trainable_variables())  # take gradients of cosst w.r.t. ALL trainable variables\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm) # clip the gradients by a pre-defined max norm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, trainables)) # add the clipped gradients to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anneal the learning rate\n",
    "Optimizers accept both scalars and tensors as learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.train.exponential_decay(init_lr,\n",
    " global_step,\n",
    " decay_steps,\n",
    " decay_rate,\n",
    " staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Use dropout through tf.nn.dropout or DropoutWrapper for cells\n",
    "- tf.nn.dropout\n",
    "\n",
    "      hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "- DropoutWrapper\n",
    "\n",
    "      cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "      cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "- Early stopping\n",
    "  - implement Early Stopping yourself by evaluating your model’s performance on a validation set every N steps during training, and saving a “winner” snapshot of the model (using a Saver) when the model outperforms the previous “winner” snapshot. At the end of training, just restore the last “winner” snapshot. Note that you should not stop immediately when performance starts dropping, because it may get better a few steps later. One good strategy is to count the number of steps since the last time a “winner” snapshot was saved, and stop when this counter is large enough that you can be confident that the network is never going to beat it.\n",
    "\n",
    "  - Another option is to use TensorFlow’s `ValidationMonitor` class and set its `early_stopping` parameters. This is documented here\n",
    "  - Or http://mckinziebrandon.me/TensorflowNotebooks/2016/11/20/early-stopping.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "n-grams: predict the next word based on previous n-grams\n",
    "- Huge vocabulary\n",
    "- Can’t generalize to OOV (out of vocabulary)\n",
    "- Requires a lot of memory\n",
    "\n",
    "Character-level: Both input and output are characters\n",
    "- Pros:\n",
    "  - Very small vocabulary\n",
    "  - Doesn’t require word embeddings\n",
    "  - Faster to train\n",
    "- Cons:\n",
    "  - Low fluency (many words can be gibberish)\n",
    "\n",
    "Word-level: \n",
    "\n",
    "Subword-level: Input and output are subwords\n",
    "- Keep W most frequent words\n",
    "- Keep S most frequent syllables\n",
    "- Split the rest into characters\n",
    "- Seem to perform better than both word-level and character-level models*\n",
    "        \n",
    "      new company dreamworks interactive\n",
    "      new company dre+ am+ wo+ rks: in+ te+ ra+ cti+ ve:\n",
    "\n",
    "Mikolov, Tomáš, et al. \"Subword language modeling with neural networks.\" (2012).\n",
    "\n",
    "Hybrid: Word-level by default, switching to character-level for unknown tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
