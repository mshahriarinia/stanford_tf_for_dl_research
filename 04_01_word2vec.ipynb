{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "For a good tutorial take a look at Tensorflow's original tutorial on this topic: https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "## Original Lecture Note Link\n",
    "Motivation and explanation of worc2vec [Stanford NLP Slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)\n",
    "\n",
    "Original Mikolov et. al. papers\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "A simple explanation on skip-gram model [blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "On Softmax, hierarchical softmax, negative sampling and NCE\n",
    "- [On word embeddings - Part 2: Approximating the Softmax](http://ruder.io/word-embeddings-softmax/)\n",
    "- [Intuition behind NCE (Noise Contrastive Estimation) for word embeddings](https://twitter.com/MShahriariNia/status/908080372298031104): Negative sampling, as the  name suggests, belongs to the family of sampling-based approaches. This family also includes importance sampling and target sampling. Negative sampling is actually a simplified model of an approach called Noise Contrastive Estimation (NCE), e.g. negative sampling makes certain assumption about the number of noise samples to generate (k) and the distribution of noise samples (Q) (negative sampling assumes that kQ(w) = 1) to simplify computation\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf):  Mikolov et al.\n",
    " have shown training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax\n",
    "\n",
    "## Summary\n",
    "\n",
    "Traditional way of doing wordembedding would be through __counting__:\n",
    "- Build a square matrix with dimmensionality of vocabulary where each item is the count. Then do matrix factorization and get an array of doubles for each token. \n",
    "\n",
    "Using SVD to find a low rank approximation. Instead of taking the full |V| we take the top k.\n",
    "\n",
    "This is similar to glove which perdicts matrices itself.\n",
    "\n",
    "CBOW has a overlapping sliding window of size |w|\n",
    "1. take dot product of one hot vector to weight matrix to get the embedding of the word\n",
    "2. calculate the dot product of the word's embedding to all the words to get the distnce to each \n",
    "3. to convert distances to probbaility distribution calculate the softmax on top of it\n",
    "4. if two words are similar teh probability distribution of all the surrounding text would be roughly similar, hence the network training idea\n",
    "\n",
    "In the basic word2vec we have a softmax, but the summation in softmax over the entire vocabulary is expensive:\n",
    "\n",
    "- Prob(output|context) = exp(u0 . v_c) / sum_w exp(u_w . v_c)  , where v_c is the context. [Reference](https://arxiv.org/pdf/1410.8251.pdf)\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "- instead of full softmax use hierarchical softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_DIR = './graphs/'\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = 'data/'\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "# $ wc text8\n",
    "# 0  17005207 100000000 text8\n",
    "# this means that text8 is a one line file with 17005207 words and 100000000 characters\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    make_dir('data')\n",
    "    print(\"Downloading data \" + DOWNLOAD_URL + file_name)\n",
    "    print(\"Data to be saved in: \" + file_path)\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception('File ' + file_name +\n",
    "                        ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return file_path\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\" Read data into a list of tokens \n",
    "    There should be 17,005,207 tokens\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split() \n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words and index of each vocab based on its count\"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]  # initialize dict of words and their count. \n",
    "    \n",
    "    # Find the k most common words and add them to the list that already has UNK\n",
    "    print(\"Calculating most common words\") # this takes time\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1)) \n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    print(\"For this experiment just use vocab_size 1000.\")\n",
    "    with open(os.path.join(LOG_DIR, 'vocab_1000.tsv'), \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            #if index < 1000:\n",
    "            if index < vocab_size:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    #index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary#, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words] # 0  mean UNK, cool!\n",
    "\n",
    "# TODO: Check\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. (Overlapping slide of variable \n",
    "    length window size. \n",
    "    yields on all the tokens both before and after) \"\"\"\n",
    "    print(\"generate_sample\")\n",
    "    print(index_words)\n",
    "    for index, center in enumerate(index_words):\n",
    "        # for each word in vocabulary generate a random sample around it with length random([0, context_window_size])\n",
    "        context = random.randint(1, context_window_size)  \n",
    "        # get a target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "        # yields on all the tokens both before and after. Always return a tuple: The center token and the current target \n",
    "\n",
    "# TODO: Check\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    print(\"get_batch\")\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def process_data(vocab_size, batch_size, skip_window):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    #dictionary, _ = build_vocab(words, vocab_size)\n",
    "    dictionary = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary) # this takes time\n",
    "    del words # to save memory\n",
    "    single_gen = generate_sample(index_words, skip_window)\n",
    "    return get_batch(single_gen, batch_size)\n",
    "\n",
    "#def get_index_vocab(vocab_size):\n",
    "#    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "#    words = read_data(file_path)\n",
    "#    return build_vocab(words, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and constants\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "NUM_SAMPLED = 64    # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "SKIP_STEP = 2000 # how many steps to skip before reporting the loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "Calculating most common words\n",
      "For this experiment just use vocab_size 1000.\n"
     ]
    }
   ],
   "source": [
    "batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_batch\n",
      "generate_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1999:  41.2\n",
      "Average loss at step 3999:   8.4\n",
      "Average loss at step 5999:   5.8\n",
      "Average loss at step 7999:   5.1\n",
      "Average loss at step 9999:   4.8\n",
      "Average loss at step 11999:   4.7\n",
      "Average loss at step 13999:   4.6\n",
      "Average loss at step 15999:   4.5\n",
      "Average loss at step 17999:   4.4\n",
      "Average loss at step 19999:   4.6\n",
      "Average loss at step 21999:   4.5\n",
      "Average loss at step 23999:   4.5\n",
      "Average loss at step 25999:   4.5\n",
      "Average loss at step 27999:   4.5\n",
      "Average loss at step 29999:   4.4\n",
      "Average loss at step 31999:   4.5\n",
      "Average loss at step 33999:   4.5\n",
      "Average loss at step 35999:   4.5\n",
      "Average loss at step 37999:   4.4\n",
      "Average loss at step 39999:   4.4\n",
      "Average loss at step 41999:   4.4\n",
      "Average loss at step 43999:   4.4\n",
      "Average loss at step 45999:   4.5\n",
      "Average loss at step 47999:   4.4\n",
      "Average loss at step 49999:   4.4\n",
      "Average loss at step 51999:   4.4\n",
      "Average loss at step 53999:   4.4\n",
      "Average loss at step 55999:   4.4\n",
      "Average loss at step 57999:   4.5\n",
      "Average loss at step 59999:   4.4\n",
      "Average loss at step 61999:   4.5\n",
      "Average loss at step 63999:   4.4\n",
      "Average loss at step 65999:   4.2\n",
      "Average loss at step 67999:   4.4\n",
      "Average loss at step 69999:   4.4\n",
      "Average loss at step 71999:   4.4\n",
      "Average loss at step 73999:   4.4\n",
      "Average loss at step 75999:   4.4\n",
      "Average loss at step 77999:   4.4\n",
      "Average loss at step 79999:   4.3\n",
      "Average loss at step 81999:   4.3\n",
      "Average loss at step 83999:   4.4\n",
      "Average loss at step 85999:   4.4\n",
      "Average loss at step 87999:   4.4\n",
      "Average loss at step 89999:   4.4\n",
      "Average loss at step 91999:   4.4\n",
      "Average loss at step 93999:   4.4\n",
      "Average loss at step 95999:   4.4\n",
      "Average loss at step 97999:   4.3\n",
      "Average loss at step 99999:   4.3\n"
     ]
    }
   ],
   "source": [
    "def word2vec(batch_gen):\n",
    "    \"\"\" Build the graph for word2vec model and train it \"\"\"\n",
    "    # Step 1: define the placeholders for input and output\n",
    "    # center_words have to be int to work on embedding lookup\n",
    "\n",
    "    with tf.name_scope('data'):\n",
    "        X = tf.placeholder(tf.int32, [BATCH_SIZE], name=\"X_CenterWords\")  # input word's index\n",
    "        Y = tf.placeholder(tf.int32, [BATCH_SIZE,1], name=\"Y_TargetWords\")  # output word's index. note: [,1]\n",
    "\n",
    "    # Step 2: define weights. In word2vec, it's actually the weights that we care about\n",
    "    # vocab size x embed size\n",
    "    # initialized to random uniform -1 to 1\n",
    "    with tf.name_scope('embedding_matrix'):\n",
    "        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE],minval=-1, maxval=1), name=\"embed_matrix\") # used for lookup via index\n",
    "\n",
    "    # Step 3: define the inference\n",
    "    # get the embed of input words using tf.nn.embedding_lookup\n",
    "    # embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "    with tf.name_scope('loss'):\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, X, name='embed')\n",
    "\n",
    "        # Step 4: construct variables for NCE loss\n",
    "        # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "        # nce_weight (vocab size x embed size), intialized to truncated_normal stddev=1.0 / (EMBED_SIZE ** 0.5)\n",
    "        # bias: vocab size, initialized to 0\n",
    "        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / (EMBED_SIZE ** 0.5)), name=\"nce_weight\")\n",
    "        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name=\"nce_bias\")\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "        # need to get the mean accross the batch\n",
    "        # note: you should use embedding of center words for inputs, not center words themselves\n",
    "        #\n",
    "        # nce_loss params:\n",
    "        #     labels: A `Tensor` of type `int64` and shape `[batch_size, num_true]`. The target classes.  Note that this format differs from\n",
    "        #             the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n",
    "        #     inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "        #     num_sampled: An `int`.  The number of classes to randomly sample per batch.\n",
    "        #     num_classes: An `int`. The number of possible classes.\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                             biases=nce_bias, \n",
    "                                             labels=Y, # index of the target word\n",
    "                                             inputs=embed,      # input embedding\n",
    "                                             num_sampled=NUM_SAMPLED, # 64 negative samples\n",
    "                                             num_classes=VOCAB_SIZE   # num_classes=VOCAB_SIZE \n",
    "                                            ), \n",
    "                                             name='loss')\n",
    "        \n",
    "    # Step 5: define optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            centers, targets = next(batch_gen)\n",
    "            \n",
    "            loss_batch, _ = sess.run([loss, optimizer], \n",
    "                                     feed_dict={X: centers, Y: targets})\n",
    "\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()\n",
    "        \n",
    "        ##\n",
    "        # save the model for visualizing embedidngs\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), NUM_TRAIN_STEPS)\n",
    "        \n",
    "        # Add meta data as labels for embeddings\n",
    "        from tensorflow.contrib.tensorboard.plugins import projector\n",
    "        # Use the same LOG_DIR where you stored your checkpoint.\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "        # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "        config = projector.ProjectorConfig()\n",
    "\n",
    "        # You can add multiple embeddings. Here we add only one.\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = embed_matrix.name\n",
    "        # Link this tensor to its metadata file (e.g. labels).\n",
    "        embedding.metadata_path = 'vocab_1000.tsv'\n",
    "\n",
    "        # Saves a configuration file that TensorBoard will read during startup.\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "\n",
    "\n",
    "word2vec(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
