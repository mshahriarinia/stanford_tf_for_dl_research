{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Lecture Note Link\n",
    "Motivation and explanation of worc2vec [Stanford NLP Slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)\n",
    "\n",
    "Original Mikolov et. al. papers\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "A simple explanation on skip-gram model [blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "On Softmax, hierarchical softmax, negative sampling and NCE\n",
    "- [On word embeddings - Part 2: Approximating the Softmax](http://ruder.io/word-embeddings-softmax/)\n",
    "- [Intuition behind NCE (Noise Contrastive Estimation) for word embeddings](https://twitter.com/MShahriariNia/status/908080372298031104): Negative sampling, as the  name suggests, belongs to the family of sampling-based approaches. This family also includes importance sampling and target sampling. Negative sampling is actually a simplified model of an approach called Noise Contrastive Estimation (NCE), e.g. negative sampling makes certain assumption about the number of noise samples to generate (k) and the distribution of noise samples (Q) (negative sampling assumes that kQ(w) = 1) to simplify computation\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf):  Mikolov et al.\n",
    " have shown training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax\n",
    "\n",
    "## Summary\n",
    "\n",
    "Traditional way of doing wordembedding would be through __counting__:\n",
    "- Build a square matrix with dimmensionality of vocabulary where each item is the count. Then do matrix factorization and get an array of doubles for each token. \n",
    "\n",
    "Using SVD to find a low rank approximation. Instead of taking the full |V| we take the top k.\n",
    "\n",
    "This is similar to glove which perdicts matrices itself.\n",
    "\n",
    "CBOW has a overlapping sliding window of size |w|\n",
    "1. take dot product of one hot vector to weight matrix to get the embedding of the word\n",
    "2. calculate the dot product of the word's embedding to all the words to get the distnce to each \n",
    "3. to convert distances to probbaility distribution calculate the softmax on top of it\n",
    "4. if two words are similar teh probability distribution of all the surrounding text would be roughly similar, hence the network training idea\n",
    "\n",
    "In the basic word2vec we have a softmax, but the summation in softmax over the entire vocabulary is expensive:\n",
    "\n",
    "- Prob(output|context) = exp(u0 . v_c) / sum_w exp(u_w . v_c)  , where v_c is the context. [Reference](https://arxiv.org/pdf/1410.8251.pdf)\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "- instead of full softmax use hierarchical softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = 'data/'\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception('File ' + file_name +\n",
    "                        ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return file_path\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\" Read data into a list of tokens \n",
    "    There should be 17,005,207 tokens\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split() \n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]  # initialize dict of words and their count. \n",
    "    \n",
    "    # Find the k most common words and add them to the list that already has UNK\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1)) \n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words] # 0  mean UNK, cool!\n",
    "\n",
    "# TODO: Check\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. (Overlapping slide of window) \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # get a random target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a random target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "\n",
    "# TODO: Check\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def process_data(vocab_size, batch_size, skip_window):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    dictionary, _ = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words # to save memory\n",
    "    single_gen = generate_sample(index_words, skip_window)\n",
    "    return get_batch(single_gen, batch_size)\n",
    "\n",
    "def get_index_vocab(vocab_size):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    return build_vocab(words, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named process_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61d070cd3e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprocess_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named process_data"
     ]
    }
   ],
   "source": [
    "# imports and constants\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from process_data import process_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
