{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN Architecture for MNIST\n",
    "conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "(W−F+2P)/S+ 1\n",
    "- W: input width\n",
    "- F: filter width\n",
    "- P: padding\n",
    "- S: stride\n",
    "\n",
    "__Variable scope:__\n",
    "Since we’ll be dealing with multiple layers, it’s important to introduce variable scope. Think of a\n",
    "variable scope something similar to a namespace. A variable name ‘weights’ in variable scope\n",
    "‘conv1’ will become ‘conv1-weights’. The common practice is to create a variable scope for each\n",
    "layer, so that if you have variable ‘weights’ in both convolution layer 1 and convolution layer 2,\n",
    "there won’t be any name clash.\n",
    "\n",
    "In variable scope, we don’t create variable using tf.Variable, but instead use tf.get_variable()\n",
    "tf.get_variable(<name>, <shape> , <initializer>)\n",
    "\n",
    "If a variable with that name already exists in that variable scope, we use that variable. If a\n",
    "variable with that name doesn’t already exists in that variable scope, TensorFlow creates a new\n",
    "variable. This setup makes it really easy to share variables across architecture. This will come in\n",
    "extremely handy when you build complex models and you need to share large sets of variables.\n",
    "Variable scopes help you initialize all of them in one place.\n",
    "\n",
    "also look into name scope.\n",
    "\n",
    "__Note:__ You can view progress in tensorboard via using summaries. http://localhost:6006/#scalars&_ignoreYOutliers=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import time \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "\n",
    "make_dir('checkpoints')\n",
    "make_dir('checkpoints/convnet_mnist')\n",
    "\n",
    "# Step 1: load MNIST data to data/mnist\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "N_CLASSES = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create placeholders for features and labels\n",
    "# - Each image in the MNIST data is of shape 28*28 = 784 therefore, each image is represented with a 1x784 tensor\n",
    "# - We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too\n",
    "# - Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__\n",
    "- Use None for shape so we can change the batch_size once we've built the graph\n",
    "- One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 42589.0\n",
      "Average loss at step 20: 21760.8\n",
      "Average loss at step 30: 14020.6\n",
      "Average loss at step 40: 9496.5\n",
      "Average loss at step 50: 7276.4\n",
      "Average loss at step 60: 5854.2\n",
      "Average loss at step 70: 4894.1\n",
      "Average loss at step 80: 4383.2\n",
      "Average loss at step 90: 3763.6\n",
      "Average loss at step 100: 3655.3\n",
      "Average loss at step 110: 3185.3\n",
      "Average loss at step 120: 2835.8\n",
      "Average loss at step 130: 2413.3\n",
      "Average loss at step 140: 2292.3\n",
      "Average loss at step 150: 1940.4\n",
      "Average loss at step 160: 2089.3\n",
      "Average loss at step 170: 1870.6\n",
      "Average loss at step 180: 1713.2\n",
      "Average loss at step 190: 1838.5\n",
      "Average loss at step 200: 1476.3\n",
      "Average loss at step 210: 1223.1\n",
      "Average loss at step 220: 1586.0\n",
      "Average loss at step 230: 1436.7\n",
      "Average loss at step 240: 1407.7\n",
      "Average loss at step 250: 1309.5\n",
      "Average loss at step 260: 1528.3\n",
      "Average loss at step 270: 1019.3\n",
      "Average loss at step 280: 1066.6\n",
      "Average loss at step 290: 1061.9\n",
      "Average loss at step 300: 972.7\n",
      "Average loss at step 310: 1102.4\n",
      "Average loss at step 320: 812.7\n",
      "Average loss at step 330: 962.6\n",
      "Average loss at step 340: 879.9\n",
      "Average loss at step 350: 808.3\n",
      "Average loss at step 360: 741.8\n",
      "Average loss at step 370: 756.3\n",
      "Average loss at step 380: 892.7\n",
      "Average loss at step 390: 720.6\n",
      "Average loss at step 400: 801.2\n",
      "Average loss at step 410: 684.1\n",
      "Average loss at step 420: 763.0\n",
      "Average loss at step 430: 906.8\n",
      "Average loss at step 440: 610.6\n",
      "Average loss at step 450: 552.7\n",
      "Average loss at step 460: 546.5\n",
      "Average loss at step 470: 501.1\n",
      "Average loss at step 480: 609.0\n",
      "Average loss at step 490: 706.7\n",
      "Average loss at step 500: 512.7\n",
      "Average loss at step 510: 480.1\n",
      "Average loss at step 520: 505.9\n",
      "Average loss at step 530: 397.6\n",
      "Average loss at step 540: 501.0\n",
      "Average loss at step 550: 603.7\n",
      "Average loss at step 560: 366.9\n",
      "Average loss at step 570: 506.2\n",
      "Average loss at step 580: 560.1\n",
      "Average loss at step 590: 402.5\n",
      "Average loss at step 600: 481.5\n",
      "Average loss at step 610: 404.1\n",
      "Average loss at step 620: 431.9\n",
      "Average loss at step 630: 498.6\n",
      "Average loss at step 640: 604.7\n",
      "Average loss at step 650: 431.6\n",
      "Average loss at step 660: 424.2\n",
      "Average loss at step 670: 296.0\n",
      "Average loss at step 680: 492.6\n",
      "Average loss at step 690: 372.6\n",
      "Average loss at step 700: 408.7\n",
      "Average loss at step 710: 263.6\n",
      "Average loss at step 720: 349.4\n",
      "Average loss at step 730: 470.3\n",
      "Average loss at step 740: 407.4\n",
      "Average loss at step 750: 438.6\n",
      "Average loss at step 760: 304.5\n",
      "Average loss at step 770: 414.6\n",
      "Average loss at step 780: 397.2\n",
      "Average loss at step 790: 462.9\n",
      "Average loss at step 800: 241.6\n",
      "Average loss at step 810: 473.0\n",
      "Average loss at step 820: 354.4\n",
      "Average loss at step 830: 296.9\n",
      "Average loss at step 840: 302.9\n",
      "Average loss at step 850: 394.7\n",
      "Average loss at step 860: 263.9\n",
      "Average loss at step 870: 282.2\n",
      "Average loss at step 880: 307.5\n",
      "Average loss at step 890: 271.2\n",
      "Average loss at step 900: 256.9\n",
      "Average loss at step 910: 263.8\n",
      "Average loss at step 920: 238.3\n",
      "Average loss at step 930: 259.1\n",
      "Average loss at step 940: 244.2\n",
      "Average loss at step 950: 238.1\n",
      "Average loss at step 960: 209.6\n",
      "Average loss at step 970: 248.2\n",
      "Average loss at step 980: 252.7\n",
      "Average loss at step 990: 292.2\n",
      "Average loss at step 1000: 286.6\n",
      "Average loss at step 1010: 216.3\n",
      "Average loss at step 1020: 197.1\n",
      "Average loss at step 1030: 278.7\n",
      "Average loss at step 1040: 299.7\n",
      "Average loss at step 1050: 255.0\n",
      "Average loss at step 1060: 261.3\n",
      "Average loss at step 1070: 203.7\n",
      "Average loss at step 1080: 249.7\n",
      "Average loss at step 1090: 293.8\n",
      "Average loss at step 1100: 268.7\n",
      "Average loss at step 1110: 215.2\n",
      "Average loss at step 1120: 238.0\n",
      "Average loss at step 1130: 177.2\n",
      "Average loss at step 1140: 167.4\n",
      "Average loss at step 1150: 173.9\n",
      "Average loss at step 1160: 214.1\n",
      "Average loss at step 1170: 266.5\n",
      "Average loss at step 1180: 209.8\n",
      "Average loss at step 1190: 218.7\n",
      "Average loss at step 1200: 216.5\n",
      "Average loss at step 1210: 234.1\n",
      "Average loss at step 1220: 274.4\n",
      "Average loss at step 1230: 231.7\n",
      "Average loss at step 1240: 197.3\n",
      "Average loss at step 1250: 222.5\n",
      "Average loss at step 1260: 160.6\n",
      "Average loss at step 1270: 163.7\n",
      "Average loss at step 1280: 234.7\n",
      "Average loss at step 1290: 169.7\n",
      "Average loss at step 1300: 155.4\n",
      "Average loss at step 1310: 137.4\n",
      "Average loss at step 1320: 110.6\n",
      "Average loss at step 1330: 197.5\n",
      "Average loss at step 1340: 129.8\n",
      "Average loss at step 1350: 156.9\n",
      "Average loss at step 1360: 154.0\n",
      "Average loss at step 1370: 126.6\n",
      "Average loss at step 1380: 111.2\n",
      "Average loss at step 1390: 152.3\n",
      "Average loss at step 1400: 145.6\n",
      "Average loss at step 1410: 162.7\n",
      "Average loss at step 1420: 170.3\n",
      "Average loss at step 1430: 198.0\n",
      "Average loss at step 1440: 161.2\n",
      "Average loss at step 1450: 179.4\n",
      "Average loss at step 1460: 137.4\n",
      "Average loss at step 1470: 182.2\n",
      "Average loss at step 1480: 121.5\n",
      "Average loss at step 1490: 126.9\n",
      "Average loss at step 1500: 132.7\n",
      "Average loss at step 1510: 171.2\n",
      "Average loss at step 1520: 161.8\n",
      "Average loss at step 1530: 118.2\n",
      "Average loss at step 1540: 134.2\n",
      "Average loss at step 1550: 171.3\n",
      "Average loss at step 1560: 132.8\n",
      "Average loss at step 1570: 156.1\n",
      "Average loss at step 1580: 115.0\n",
      "Average loss at step 1590: 167.6\n",
      "Average loss at step 1600: 123.5\n",
      "Average loss at step 1610: 130.1\n",
      "Average loss at step 1620: 147.5\n",
      "Average loss at step 1630: 127.9\n",
      "Average loss at step 1640: 137.7\n",
      "Average loss at step 1650: 127.3\n",
      "Average loss at step 1660: 135.6\n",
      "Average loss at step 1670: 125.8\n",
      "Average loss at step 1680: 151.9\n",
      "Average loss at step 1690: 203.3\n",
      "Average loss at step 1700:  95.5\n",
      "Average loss at step 1710: 120.4\n",
      "Average loss at step 1720: 133.9\n",
      "Average loss at step 1730: 108.7\n",
      "Average loss at step 1740: 109.9\n",
      "Average loss at step 1750:  73.5\n",
      "Average loss at step 1760:  98.1\n",
      "Average loss at step 1770: 138.9\n",
      "Average loss at step 1780: 138.4\n",
      "Average loss at step 1790:  76.6\n",
      "Average loss at step 1800: 100.5\n",
      "Average loss at step 1810: 154.7\n",
      "Average loss at step 1820: 128.1\n",
      "Average loss at step 1830:  82.6\n",
      "Average loss at step 1840:  83.4\n",
      "Average loss at step 1850:  93.9\n",
      "Average loss at step 1860:  90.8\n",
      "Average loss at step 1870:  84.8\n",
      "Average loss at step 1880: 108.2\n",
      "Average loss at step 1890: 129.1\n",
      "Average loss at step 1900: 103.7\n",
      "Average loss at step 1910:  89.4\n",
      "Average loss at step 1920:  79.1\n",
      "Average loss at step 1930: 100.2\n",
      "Average loss at step 1940: 112.7\n",
      "Average loss at step 1950:  74.1\n",
      "Average loss at step 1960:  85.6\n",
      "Average loss at step 1970:  78.6\n",
      "Average loss at step 1980:  94.2\n",
      "Average loss at step 1990: 118.5\n",
      "Average loss at step 2000: 123.1\n",
      "Average loss at step 2010:  93.9\n",
      "Average loss at step 2020:  50.2\n",
      "Average loss at step 2030: 125.6\n",
      "Average loss at step 2040: 106.1\n",
      "Average loss at step 2050:  95.8\n",
      "Average loss at step 2060:  95.6\n",
      "Average loss at step 2070:  96.1\n",
      "Average loss at step 2080:  66.7\n",
      "Average loss at step 2090: 111.0\n",
      "Average loss at step 2100:  85.9\n",
      "Average loss at step 2110:  63.9\n",
      "Average loss at step 2120:  79.5\n",
      "Average loss at step 2130: 121.4\n",
      "Average loss at step 2140:  76.9\n",
      "Average loss at step 2150:  81.6\n",
      "Average loss at step 2160:  81.6\n",
      "Average loss at step 2170:  98.6\n",
      "Average loss at step 2180:  87.0\n",
      "Average loss at step 2190:  92.8\n",
      "Average loss at step 2200:  90.7\n",
      "Average loss at step 2210:  80.0\n",
      "Average loss at step 2220:  67.9\n",
      "Average loss at step 2230:  61.5\n",
      "Average loss at step 2240:  85.7\n",
      "Average loss at step 2250:  62.1\n",
      "Average loss at step 2260:  62.6\n",
      "Average loss at step 2270:  64.7\n",
      "Average loss at step 2280:  57.5\n",
      "Average loss at step 2290:  97.7\n",
      "Average loss at step 2300:  90.1\n",
      "Average loss at step 2310:  60.9\n",
      "Average loss at step 2320:  49.9\n",
      "Average loss at step 2330:  81.9\n",
      "Average loss at step 2340:  65.3\n",
      "Average loss at step 2350:  86.2\n",
      "Average loss at step 2360:  91.5\n",
      "Average loss at step 2370:  75.2\n",
      "Average loss at step 2380:  77.2\n",
      "Average loss at step 2390:  66.0\n",
      "Average loss at step 2400:  84.4\n",
      "Average loss at step 2410:  53.4\n",
      "Average loss at step 2420:  51.8\n",
      "Average loss at step 2430:  51.1\n",
      "Average loss at step 2440:  74.7\n",
      "Average loss at step 2450: 113.1\n",
      "Average loss at step 2460:  77.7\n",
      "Average loss at step 2470:  88.1\n",
      "Average loss at step 2480:  59.3\n",
      "Average loss at step 2490:  94.8\n",
      "Average loss at step 2500:  45.3\n",
      "Average loss at step 2510:  55.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2520:  74.0\n",
      "Average loss at step 2530:  80.4\n",
      "Average loss at step 2540:  64.2\n",
      "Average loss at step 2550:  52.0\n",
      "Average loss at step 2560:  77.7\n",
      "Average loss at step 2570:  50.5\n",
      "Average loss at step 2580:  56.8\n",
      "Average loss at step 2590:  52.3\n",
      "Average loss at step 2600:  74.3\n",
      "Average loss at step 2610:  44.3\n",
      "Average loss at step 2620:  59.6\n",
      "Average loss at step 2630:  66.1\n",
      "Average loss at step 2640:  48.8\n",
      "Average loss at step 2650:  56.2\n",
      "Average loss at step 2660:  29.9\n",
      "Average loss at step 2670:  48.6\n",
      "Average loss at step 2680:  48.3\n",
      "Average loss at step 2690:  58.0\n",
      "Average loss at step 2700:  43.3\n",
      "Average loss at step 2710:  65.9\n",
      "Average loss at step 2720:  42.3\n",
      "Average loss at step 2730:  46.2\n",
      "Average loss at step 2740:  49.6\n",
      "Average loss at step 2750:  46.8\n",
      "Average loss at step 2760:  66.5\n",
      "Average loss at step 2770:  49.7\n",
      "Average loss at step 2780:  41.8\n",
      "Average loss at step 2790:  68.9\n",
      "Average loss at step 2800:  37.2\n",
      "Average loss at step 2810:  30.5\n",
      "Average loss at step 2820:  53.3\n",
      "Average loss at step 2830:  86.6\n",
      "Average loss at step 2840:  61.1\n",
      "Average loss at step 2850:  51.1\n",
      "Average loss at step 2860:  44.2\n",
      "Average loss at step 2870:  33.6\n",
      "Average loss at step 2880:  72.3\n",
      "Average loss at step 2890:  84.1\n",
      "Average loss at step 2900:  61.0\n",
      "Average loss at step 2910:  52.4\n",
      "Average loss at step 2920:  59.6\n",
      "Average loss at step 2930:  43.8\n",
      "Average loss at step 2940:  46.7\n",
      "Average loss at step 2950:  44.6\n",
      "Average loss at step 2960:  64.3\n",
      "Average loss at step 2970:  44.2\n",
      "Average loss at step 2980:  56.6\n",
      "Average loss at step 2990:  70.0\n",
      "Average loss at step 3000:  71.2\n",
      "Average loss at step 3010:  52.3\n",
      "Average loss at step 3020:  33.4\n",
      "Average loss at step 3030:  35.2\n",
      "Average loss at step 3040:  31.7\n",
      "Average loss at step 3050:  40.1\n",
      "Average loss at step 3060:  60.0\n",
      "Average loss at step 3070:  46.0\n",
      "Average loss at step 3080:  32.3\n",
      "Average loss at step 3090:  37.3\n",
      "Average loss at step 3100:  56.9\n",
      "Average loss at step 3110:  46.6\n",
      "Average loss at step 3120:  54.8\n",
      "Average loss at step 3130:  44.0\n",
      "Average loss at step 3140:  24.6\n",
      "Average loss at step 3150:  37.9\n",
      "Average loss at step 3160:  21.8\n",
      "Average loss at step 3170:  32.1\n",
      "Average loss at step 3180:  32.4\n",
      "Average loss at step 3190:  43.2\n",
      "Average loss at step 3200:  25.7\n",
      "Average loss at step 3210:  65.8\n",
      "Average loss at step 3220:  47.6\n",
      "Average loss at step 3230:  30.1\n",
      "Average loss at step 3240:  51.3\n",
      "Average loss at step 3250:  45.9\n",
      "Average loss at step 3260:  30.9\n",
      "Average loss at step 3270:  45.9\n",
      "Average loss at step 3280:  35.5\n",
      "Average loss at step 3290:  42.3\n",
      "Average loss at step 3300:  50.2\n",
      "Average loss at step 3310:  53.9\n",
      "Average loss at step 3320:  32.7\n",
      "Average loss at step 3330:  60.2\n",
      "Average loss at step 3340:  47.6\n",
      "Average loss at step 3350:  35.7\n",
      "Average loss at step 3360:  49.1\n",
      "Average loss at step 3370:  35.0\n",
      "Average loss at step 3380:  47.5\n",
      "Average loss at step 3390:  28.2\n",
      "Average loss at step 3400:  55.3\n",
      "Average loss at step 3410:  20.3\n",
      "Average loss at step 3420:  37.8\n",
      "Average loss at step 3430:  47.9\n",
      "Average loss at step 3440:  38.5\n",
      "Average loss at step 3450:  44.8\n",
      "Average loss at step 3460:  23.8\n",
      "Average loss at step 3470:  43.7\n",
      "Average loss at step 3480:  55.5\n",
      "Average loss at step 3490:  35.8\n",
      "Average loss at step 3500:  14.5\n",
      "Average loss at step 3510:  33.7\n",
      "Average loss at step 3520:  45.1\n",
      "Average loss at step 3530:  30.3\n",
      "Average loss at step 3540:  47.4\n",
      "Average loss at step 3550:  39.6\n",
      "Average loss at step 3560:  35.8\n",
      "Average loss at step 3570:  40.9\n",
      "Average loss at step 3580:  39.8\n",
      "Average loss at step 3590:  16.8\n",
      "Average loss at step 3600:  24.0\n",
      "Average loss at step 3610:  45.0\n",
      "Average loss at step 3620:  24.7\n",
      "Average loss at step 3630:  27.5\n",
      "Average loss at step 3640:  37.1\n",
      "Average loss at step 3650:  34.6\n",
      "Average loss at step 3660:  26.1\n",
      "Average loss at step 3670:  41.9\n",
      "Average loss at step 3680:  35.1\n",
      "Average loss at step 3690:  39.3\n",
      "Average loss at step 3700:  16.1\n",
      "Average loss at step 3710:  36.4\n",
      "Average loss at step 3720:  32.1\n",
      "Average loss at step 3730:  24.6\n",
      "Average loss at step 3740:  21.1\n",
      "Average loss at step 3750:  21.0\n",
      "Average loss at step 3760:  29.5\n",
      "Average loss at step 3770:  45.7\n",
      "Average loss at step 3780:  32.8\n",
      "Average loss at step 3790:  23.6\n",
      "Average loss at step 3800:  29.2\n",
      "Average loss at step 3810:  29.6\n",
      "Average loss at step 3820:  34.6\n",
      "Average loss at step 3830:  25.0\n",
      "Average loss at step 3840:  30.3\n",
      "Average loss at step 3850:  43.2\n",
      "Average loss at step 3860:  32.6\n",
      "Optimization Finished!\n",
      "Total time: 2626.15314698 seconds\n",
      "Accuracy 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Step 4 + 5: create weights + do inference\n",
    "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    # use the dynamic dimension -1\n",
    "    \n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    # use tf.truncated_normal_initializer()\n",
    "    # For conv2d: Given an input tensor of shape [batch, in_height, in_width, in_channels] \n",
    "    # and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv1_kernels =  tf.get_variable(shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer(),   # YES mean=0, stddev=1 \n",
    "                                     name='conv1kernels', dtype=tf.float32)\n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # create biases variable of dimension [32]\n",
    "    # use tf.constant_initializer(0.0)\n",
    "    #conv1_biases = tf.Variable([32], tf.constant_initializer(0.0), name='conv1biases', dtype=tf.float32)  # YES\n",
    "    conv1_biases = tf.get_variable('biases', [32],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    # TO DO \n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    \n",
    "    conv1_output = tf.nn.conv2d(images, conv1_kernels, strides=[1,1,1,1], padding='SAME', name='conv1')\n",
    "    # TO DO\n",
    "\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    relu1 = tf.nn.relu(tf.add(conv1_output, conv1_biases), name='relu1')    \n",
    "    # TO DO \n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
    "    # stride of 2,2 means non-overlapping\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='maxpool1')\n",
    "    # TO DO\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 32\n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    # 32 is the input channels (comes from previous layers)\n",
    "    conv2_kernels = tf.get_variable('conv2_kernels', [5, 5, 32, 64], initializer=tf.truncated_normal_initializer())\n",
    "    conv2_biases = tf.get_variable('conv2_biases', [64], initializer=tf.random_normal_initializer()) # TODO: WHY not 0?\n",
    "    conv2_output = tf.nn.conv2d(max_pool1, conv2_kernels, strides=[1, 1, 1, 1], padding='SAME', name='conv2output')\n",
    "    relu2 = tf.nn.relu(conv2_output + conv2_biases, name='relu2')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool2')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64\n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "    # create weights and biases\n",
    "    #fc_weight = tf.Variable(tf.random_normal([input_features, 1024]), name='fcweight') YES\n",
    "    #fc_bias = tf.Variable(tf.zeros([1024]), name='fcbias') YES\n",
    "    \n",
    "    fc_weight = tf.get_variable('fc_weight', [input_features, 1024],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    fc_bias = tf.get_variable('fc_bias', [1024],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "    # TO DO\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    max_pool2 = tf.reshape(max_pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(max_pool2, fc_weight) + fc_bias, name='fcrelu')\n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='dropout')\n",
    "\n",
    "with tf.variable_scope('softmaxlinear') as scope:\n",
    "    # this you should know. get logits without softmax\n",
    "    # you need to create weights and biases\n",
    "    \n",
    "   # W = tf.Variable(tf.random_normal(shape=[1024,10], mean=0, stddev=0.1), name=\"W\") YES\n",
    "    #b = tf.Variable(tf.zeros(shape=[10]), name=\"b\") YES\n",
    "    \n",
    "    W = tf.get_variable('weights', [1024, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    logits = tf.matmul(fc, W) + b\n",
    "\n",
    "    \n",
    "\n",
    "    # TO DO\n",
    "\n",
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "with tf.name_scope('loss'):\n",
    "    # you should know how to do this too\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name=\"entropy\")\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "    # TO DO\n",
    "\n",
    "    \n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "# don't forgot to pass in global_step\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    lossOptimizer = optimizer.minimize(loss, global_step=global_step)\n",
    "# TO DO\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "    ##### You have to create folders to store checkpoints\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([lossOptimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([lossOptimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "Adam with no weight initialization: Total time: 5.83721089363 seconds Accuracy 0.8358\n",
    "\n",
    "Adam with initialization\n",
    "Average loss at step 10: 1802.0\n",
    "Average loss at step 20: 699.4\n",
    "Average loss at step 30: 445.7\n",
    "Average loss at step 40: 288.4\n",
    "Average loss at step 50: 224.2\n",
    "Average loss at step 60: 179.3\n",
    "Average loss at step 70: 166.3\n",
    "Average loss at step 80: 131.9\n",
    "Average loss at step 90: 122.3\n",
    "Average loss at step 100:  95.3\n",
    "Average loss at step 110:  89.0\n",
    "Average loss at step 120:  87.8\n",
    "Average loss at step 130:  69.9\n",
    "Average loss at step 140:  72.3\n",
    "Average loss at step 150:  56.3\n",
    "Average loss at step 160:  54.9\n",
    "Average loss at step 170:  66.5\n",
    "Average loss at step 180:  49.7\n",
    "Average loss at step 190:  52.7\n",
    "Average loss at step 200:  55.5\n",
    "Average loss at step 210:  44.3\n",
    "Average loss at step 220:  45.9\n",
    "Average loss at step 230:  46.4\n",
    "Average loss at step 240:  37.0\n",
    "Average loss at step 250:  38.6\n",
    "Average loss at step 260:  42.6\n",
    "Average loss at step 270:  34.6\n",
    "Average loss at step 280:  31.8\n",
    "Average loss at step 290:  31.8\n",
    "Average loss at step 300:  31.1\n",
    "Average loss at step 310:  31.4\n",
    "Average loss at step 320:  26.2\n",
    "Average loss at step 330:  30.7\n",
    "Average loss at step 340:  25.5\n",
    "Average loss at step 350:  26.8\n",
    "Average loss at step 360:  23.8\n",
    "Average loss at step 370:  26.7\n",
    "Average loss at step 380:  23.8\n",
    "Average loss at step 390:  21.8\n",
    "Average loss at step 400:  23.6\n",
    "Average loss at step 410:  19.2\n",
    "Average loss at step 420:  18.6\n",
    "Optimization Finished!\n",
    "Total time: 284.903088093 seconds\n",
    "Accuracy 0.9174\n",
    "\n",
    "Adam\n",
    "Average loss at step 10: 30222.2\n",
    "Average loss at step 20: 16148.8\n",
    "Average loss at step 30: 9587.3\n",
    "Average loss at step 40: 6922.9\n",
    "Average loss at step 50: 4953.3\n",
    "Average loss at step 60: 4613.0\n",
    "Average loss at step 70: 3682.1\n",
    "Average loss at step 80: 3718.2\n",
    "Average loss at step 90: 3408.3\n",
    "Average loss at step 100: 2872.0\n",
    "Average loss at step 110: 2445.8\n",
    "Average loss at step 120: 2078.3\n",
    "Average loss at step 130: 2231.1\n",
    "Average loss at step 140: 1894.4\n",
    "Average loss at step 150: 1677.1\n",
    "Average loss at step 160: 2162.8\n",
    "Average loss at step 170: 1588.3\n",
    "Average loss at step 180: 1731.8\n",
    "Average loss at step 190: 2008.5\n",
    "Average loss at step 200: 1751.3\n",
    "Average loss at step 210: 1478.8\n",
    "Average loss at step 220: 1362.8\n",
    "Average loss at step 230: 1276.3\n",
    "Average loss at step 240: 1247.8\n",
    "Average loss at step 250: 1045.4\n",
    "Average loss at step 260: 1269.3\n",
    "Average loss at step 270: 1031.2\n",
    "Average loss at step 280: 1283.8\n",
    "Average loss at step 290: 1029.7\n",
    "Average loss at step 300: 861.0\n",
    "Average loss at step 310: 1002.9\n",
    "Average loss at step 320: 837.7\n",
    "Average loss at step 330: 823.5\n",
    "Average loss at step 340: 1034.0\n",
    "Average loss at step 350: 1112.5\n",
    "Average loss at step 360: 791.2\n",
    "Average loss at step 370: 940.1\n",
    "Average loss at step 380: 802.0\n",
    "Average loss at step 390: 640.3\n",
    "Average loss at step 400: 635.3\n",
    "Average loss at step 410: 806.9\n",
    "Average loss at step 420: 781.4\n",
    "Optimization Finished!\n",
    "Total time: 309.968480825 seconds\n",
    "Accuracy 0.9208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "Big blockers were:\n",
    "1. Good optmizer (Adam vs Stochastic gradient descent)\n",
    "2. Proper random initialization\n",
    "3. Use of lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
