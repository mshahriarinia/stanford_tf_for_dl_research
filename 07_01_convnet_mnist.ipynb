{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN Architecture for MNIST\n",
    "conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "(W−F+2P)/S+ 1\n",
    "- W: input width\n",
    "- F: filter width\n",
    "- P: padding\n",
    "- S: stride\n",
    "\n",
    "__Variable scope:__\n",
    "Since we’ll be dealing with multiple layers, it’s important to introduce variable scope. Think of a\n",
    "variable scope something similar to a namespace. A variable name ‘weights’ in variable scope\n",
    "‘conv1’ will become ‘conv1-weights’. The common practice is to create a variable scope for each\n",
    "layer, so that if you have variable ‘weights’ in both convolution layer 1 and convolution layer 2,\n",
    "there won’t be any name clash.\n",
    "\n",
    "In variable scope, we don’t create variable using tf.Variable, but instead use tf.get_variable()\n",
    "tf.get_variable(<name>, <shape> , <initializer>)\n",
    "\n",
    "If a variable with that name already exists in that variable scope, we use that variable. If a\n",
    "variable with that name doesn’t already exists in that variable scope, TensorFlow creates a new\n",
    "variable. This setup makes it really easy to share variables across architecture. This will come in\n",
    "extremely handy when you build complex models and you need to share large sets of variables.\n",
    "Variable scopes help you initialize all of them in one place.\n",
    "\n",
    "also look into name scope.\n",
    "\n",
    "__Note:__ You can view progress in tensorboard via using summaries. http://localhost:6006/#scalars&_ignoreYOutliers=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import time \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "\n",
    "make_dir('checkpoints')\n",
    "make_dir('checkpoints/convnet_mnist')\n",
    "\n",
    "# Step 1: load MNIST data to data/mnist\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "N_CLASSES = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create placeholders for features and labels\n",
    "# - Each image in the MNIST data is of shape 28*28 = 784 therefore, each image is represented with a 1x784 tensor\n",
    "# - We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too\n",
    "# - Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__\n",
    "- Use None for shape so we can change the batch_size once we've built the graph\n",
    "- One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 36946.6\n",
      "Average loss at step 20: 21872.6\n",
      "Average loss at step 30: 13436.8\n",
      "Average loss at step 40: 9398.1\n",
      "Average loss at step 50: 6879.5\n",
      "Average loss at step 60: 5101.2\n",
      "Average loss at step 70: 4285.3\n",
      "Average loss at step 80: 4133.6\n",
      "Average loss at step 90: 3540.7\n",
      "Average loss at step 100: 3175.9\n",
      "Average loss at step 110: 2846.9\n",
      "Average loss at step 120: 2682.5\n",
      "Average loss at step 130: 2582.2\n",
      "Average loss at step 140: 1975.6\n",
      "Average loss at step 150: 2077.3\n",
      "Average loss at step 160: 1833.6\n",
      "Average loss at step 170: 1928.1\n",
      "Average loss at step 180: 1882.1\n",
      "Average loss at step 190: 1331.1\n",
      "Average loss at step 200: 1720.3\n",
      "Average loss at step 210: 1362.8\n",
      "Average loss at step 220: 1346.1\n",
      "Average loss at step 230: 1430.8\n",
      "Average loss at step 240: 1401.5\n",
      "Average loss at step 250: 1264.5\n",
      "Average loss at step 260: 1226.7\n",
      "Average loss at step 270: 1130.2\n",
      "Average loss at step 280: 1046.6\n",
      "Average loss at step 290: 873.9\n",
      "Average loss at step 300: 1140.8\n",
      "Average loss at step 310: 951.8\n",
      "Average loss at step 320: 801.8\n",
      "Average loss at step 330: 842.3\n",
      "Average loss at step 340: 906.4\n",
      "Average loss at step 350: 915.8\n",
      "Average loss at step 360: 855.9\n",
      "Average loss at step 370: 1021.0\n",
      "Average loss at step 380: 654.9\n",
      "Average loss at step 390: 798.7\n",
      "Average loss at step 400: 907.4\n",
      "Average loss at step 410: 679.8\n",
      "Average loss at step 420: 834.7\n",
      "Optimization Finished!\n",
      "Total time: 368.334231853 seconds\n",
      "Accuracy 0.9159\n"
     ]
    }
   ],
   "source": [
    "# Step 4 + 5: create weights + do inference\n",
    "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    # use the dynamic dimension -1\n",
    "    \n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    # use tf.truncated_normal_initializer()\n",
    "    # For conv2d: Given an input tensor of shape [batch, in_height, in_width, in_channels] \n",
    "    # and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv1_kernels =  tf.get_variable(shape=[5, 5, 1, 32], initializer=tf.truncated_normal_initializer(),   # YES mean=0, stddev=1 \n",
    "                                     name='conv1kernels', dtype=tf.float32)\n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # create biases variable of dimension [32]\n",
    "    # use tf.constant_initializer(0.0)\n",
    "    #conv1_biases = tf.Variable([32], tf.constant_initializer(0.0), name='conv1biases', dtype=tf.float32)  # YES\n",
    "    conv1_biases = tf.get_variable('biases', [32],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    # TO DO \n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    \n",
    "    conv1_output = tf.nn.conv2d(images, conv1_kernels, strides=[1,1,1,1], padding='SAME', name='conv1')\n",
    "    # TO DO\n",
    "\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    relu1 = tf.nn.relu(tf.add(conv1_output, conv1_biases), name='relu1')    \n",
    "    # TO DO \n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
    "    # stride of 2,2 means non-overlapping\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='maxpool1')\n",
    "    # TO DO\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 32\n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    # 32 is the input channels (comes from previous layers)\n",
    "    conv2_kernels = tf.get_variable('conv2_kernels', [5, 5, 32, 64], initializer=tf.truncated_normal_initializer())\n",
    "    conv2_biases = tf.get_variable('conv2_biases', [64], initializer=tf.random_normal_initializer()) # TODO: WHY not 0?\n",
    "    conv2_output = tf.nn.conv2d(max_pool1, conv2_kernels, strides=[1, 1, 1, 1], padding='SAME', name='conv2output')\n",
    "    relu2 = tf.nn.relu(conv2_output + conv2_biases, name='relu2')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool2')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64\n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "    # create weights and biases\n",
    "    #fc_weight = tf.Variable(tf.random_normal([input_features, 1024]), name='fcweight') YES\n",
    "    #fc_bias = tf.Variable(tf.zeros([1024]), name='fcbias') YES\n",
    "    \n",
    "    fc_weight = tf.get_variable('fc_weight', [input_features, 1024],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    fc_bias = tf.get_variable('fc_bias', [1024],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "    # TO DO\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    max_pool2 = tf.reshape(max_pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(max_pool2, fc_weight) + fc_bias, name='fcrelu')\n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='dropout')\n",
    "\n",
    "with tf.variable_scope('softmaxlinear') as scope:\n",
    "    # this you should know. get logits without softmax\n",
    "    # you need to create weights and biases\n",
    "    \n",
    "   # W = tf.Variable(tf.random_normal(shape=[1024,10], mean=0, stddev=0.1), name=\"W\") YES\n",
    "    #b = tf.Variable(tf.zeros(shape=[10]), name=\"b\") YES\n",
    "    \n",
    "    W = tf.get_variable('weights', [1024, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    logits = tf.matmul(fc, W) + b\n",
    "\n",
    "    \n",
    "\n",
    "    # TO DO\n",
    "\n",
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "with tf.name_scope('loss'):\n",
    "    # you should know how to do this too\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name=\"entropy\")\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "    # TO DO\n",
    "\n",
    "    \n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "# don't forgot to pass in global_step\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    lossOptimizer = optimizer.minimize(loss, global_step=global_step)\n",
    "# TO DO\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "    ##### You have to create folders to store checkpoints\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([lossOptimizer, loss, summary_op], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([lossOptimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "Adam with no weight initialization: Total time: 5.83721089363 seconds Accuracy 0.8358\n",
    "\n",
    "Adam with initialization\n",
    "Average loss at step 10: 1802.0\n",
    "Average loss at step 20: 699.4\n",
    "Average loss at step 30: 445.7\n",
    "Average loss at step 40: 288.4\n",
    "Average loss at step 50: 224.2\n",
    "Average loss at step 60: 179.3\n",
    "Average loss at step 70: 166.3\n",
    "Average loss at step 80: 131.9\n",
    "Average loss at step 90: 122.3\n",
    "Average loss at step 100:  95.3\n",
    "Average loss at step 110:  89.0\n",
    "Average loss at step 120:  87.8\n",
    "Average loss at step 130:  69.9\n",
    "Average loss at step 140:  72.3\n",
    "Average loss at step 150:  56.3\n",
    "Average loss at step 160:  54.9\n",
    "Average loss at step 170:  66.5\n",
    "Average loss at step 180:  49.7\n",
    "Average loss at step 190:  52.7\n",
    "Average loss at step 200:  55.5\n",
    "Average loss at step 210:  44.3\n",
    "Average loss at step 220:  45.9\n",
    "Average loss at step 230:  46.4\n",
    "Average loss at step 240:  37.0\n",
    "Average loss at step 250:  38.6\n",
    "Average loss at step 260:  42.6\n",
    "Average loss at step 270:  34.6\n",
    "Average loss at step 280:  31.8\n",
    "Average loss at step 290:  31.8\n",
    "Average loss at step 300:  31.1\n",
    "Average loss at step 310:  31.4\n",
    "Average loss at step 320:  26.2\n",
    "Average loss at step 330:  30.7\n",
    "Average loss at step 340:  25.5\n",
    "Average loss at step 350:  26.8\n",
    "Average loss at step 360:  23.8\n",
    "Average loss at step 370:  26.7\n",
    "Average loss at step 380:  23.8\n",
    "Average loss at step 390:  21.8\n",
    "Average loss at step 400:  23.6\n",
    "Average loss at step 410:  19.2\n",
    "Average loss at step 420:  18.6\n",
    "Optimization Finished!\n",
    "Total time: 284.903088093 seconds\n",
    "Accuracy 0.9174\n",
    "\n",
    "Adam\n",
    "Average loss at step 10: 30222.2\n",
    "Average loss at step 20: 16148.8\n",
    "Average loss at step 30: 9587.3\n",
    "Average loss at step 40: 6922.9\n",
    "Average loss at step 50: 4953.3\n",
    "Average loss at step 60: 4613.0\n",
    "Average loss at step 70: 3682.1\n",
    "Average loss at step 80: 3718.2\n",
    "Average loss at step 90: 3408.3\n",
    "Average loss at step 100: 2872.0\n",
    "Average loss at step 110: 2445.8\n",
    "Average loss at step 120: 2078.3\n",
    "Average loss at step 130: 2231.1\n",
    "Average loss at step 140: 1894.4\n",
    "Average loss at step 150: 1677.1\n",
    "Average loss at step 160: 2162.8\n",
    "Average loss at step 170: 1588.3\n",
    "Average loss at step 180: 1731.8\n",
    "Average loss at step 190: 2008.5\n",
    "Average loss at step 200: 1751.3\n",
    "Average loss at step 210: 1478.8\n",
    "Average loss at step 220: 1362.8\n",
    "Average loss at step 230: 1276.3\n",
    "Average loss at step 240: 1247.8\n",
    "Average loss at step 250: 1045.4\n",
    "Average loss at step 260: 1269.3\n",
    "Average loss at step 270: 1031.2\n",
    "Average loss at step 280: 1283.8\n",
    "Average loss at step 290: 1029.7\n",
    "Average loss at step 300: 861.0\n",
    "Average loss at step 310: 1002.9\n",
    "Average loss at step 320: 837.7\n",
    "Average loss at step 330: 823.5\n",
    "Average loss at step 340: 1034.0\n",
    "Average loss at step 350: 1112.5\n",
    "Average loss at step 360: 791.2\n",
    "Average loss at step 370: 940.1\n",
    "Average loss at step 380: 802.0\n",
    "Average loss at step 390: 640.3\n",
    "Average loss at step 400: 635.3\n",
    "Average loss at step 410: 806.9\n",
    "Average loss at step 420: 781.4\n",
    "Optimization Finished!\n",
    "Total time: 309.968480825 seconds\n",
    "Accuracy 0.9208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "Big blockers were:\n",
    "1. Good optmizer (Adam vs Stochastic gradient descent)\n",
    "2. Proper random initialization\n",
    "3. Use of lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
